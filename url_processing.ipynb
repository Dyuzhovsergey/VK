{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from gensim.models import Word2Vec\n",
    "import nltk\n",
    "import pickle\n",
    "import faiss\n",
    "import pandas as pd\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../data/sequences.npy', 'rb') as f:\n",
    "    urls = np.load(f, allow_pickle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Word2Vec(sentences=urls, vector_size=100, window=5, min_count=1, workers=4)\n",
    "model.save('../data/urls_vectorization.model')\n",
    "\n",
    "urls_vocab = list(model.wv.index_to_key) \n",
    "with open('../data/urls_vocab.pkl', 'wb') as f:\n",
    "    pickle.dump(urls_vocab, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "urls_vectors = np.load('../data/urls_vectorization.model.wv.vectors.npy')\n",
    "urls_vectors = urls_vectors.astype(np.float32)\n",
    "\n",
    "with open('../data/urls_vocab.pkl', 'rb') as f:\n",
    "    urls_vocab = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "clusters_number = 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kmeans = faiss.Kmeans(d=urls_vectors.shape[1], k=clusters_number, nredo=20, niter=500, gpu=True)\n",
    "kmeans.train(urls_vectors)\n",
    "\n",
    "clusters = kmeans.index.search(urls_vectors, 1)[1].astype(int).flatten()\n",
    "np.save('../data/clusters.npy', clusters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clusters_urls = [[] for _ in range(128)] \n",
    "for cluster, url in zip(clusters, urls_vocab):\n",
    "    clusters_urls[cluster].append(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../data/clustered_urls.pkl', 'rb') as f:\n",
    "    clustered_urls = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "clusters = np.load('../data/clusters.npy')\n",
    "\n",
    "with open('../data/urls_vocab.pkl', 'rb') as f:\n",
    "    urls_vocab = pickle.load(f)\n",
    "\n",
    "urls_clusters = {}\n",
    "for url, cluster in zip(urls_vocab, clusters):\n",
    "    urls_clusters[url] = cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_table('../data/train')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = data.drop(columns=['DEF', 'tokens'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X['urls_hashed'] = X['urls_hashed'].astype(str)\n",
    "X['urls_hashed'].fillna('')\n",
    "\n",
    "urls_users = {}\n",
    "\n",
    "for row in range(len(X['urls_hashed'])):\n",
    "    values = X['urls_hashed'][row].split(' ')\n",
    "    if len(values) > 1:\n",
    "        for i in range(0, len(values), 2):\n",
    "            if values[i] in urls_users:\n",
    "                urls_users[values[i]][row] = int(values[i + 1])\n",
    "            else:\n",
    "                urls_users[values[i]] = {row: int(values[i + 1])}\n",
    "\n",
    "    if row % 10000 == 0:\n",
    "        print(f'Processed {row} rows')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count = 0\n",
    "clusters_users = [{} for _ in range(clusters_number)] \n",
    "for url in urls_users:\n",
    "    if url in urls_clusters:\n",
    "        cluster = int(urls_clusters[url])\n",
    "        for user in urls_users[url]:\n",
    "            clusters_users[cluster][user] = urls_users[url][user]\n",
    "        \n",
    "    count += 1\n",
    "    if count % 10000 == 0:\n",
    "        print(f'Processed {count} urls')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../data/clusters_users.json', 'w') as f:\n",
    "    json.dump(clusters_users, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../data/clusters_users.json', 'r') as f:\n",
    "    clusters_users = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = data.drop(columns=['DEF', 'tokens', 'urls_hashed'])\n",
    "for i in range(clusters_number):\n",
    "    X[f'url_cat_{i}'] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for col_number in range(clusters_number):\n",
    "    col = f'url_cat_{col_number}'\n",
    "    count = 0\n",
    "    for row in range(len(X[col])):\n",
    "        if row in clusters_users[col_number]:\n",
    "            X.loc[row, col] = clusters_users[col_number][row] \n",
    "\n",
    "        count += 1\n",
    "        if count % 10000 == 0:\n",
    "            print(f'Processed {count} users for columnt {col_number}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0      241506\n",
      "1       40914\n",
      "2       13595\n",
      "3        2961\n",
      "4        1970\n",
      "        ...  \n",
      "127         1\n",
      "241         1\n",
      "68          1\n",
      "91          1\n",
      "254         1\n",
      "Name: url_cat_2, Length: 110, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(X['url_cat_2'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.to_csv('../data/processed_train.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.6 ('venv')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "b369af22ab1e8acbe9e1b36f6a36573e6c76df0f16966b2e7ac1f0d8fa2f44ce"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
